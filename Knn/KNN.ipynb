{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)\tExplain the working of KNN algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for a given trainng data set when we have a test data \n",
    "- we have to calculate the distance between the all the trainning data points with the test data.\n",
    "- After calculating the distance, it will select the k training points which are nearest to the test data. K is randomly choosen any number.\n",
    "- Now, k nearest neighbors are selected, if k is 2 then 2 nearest neigbours are choosen.\n",
    "- Then if it is a classificaion problem then we find out the probability based on the choosen neighbors  if it's a regression problem we have to find out the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)\tHow KNN predicts the output for regression and classification problems?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression problem we have to find out the probability of the each class based on choosen k neighbours. if it is regression problem then we find out the mean of the k neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\tWhat are the different distances used in KNN? How are they calculated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 types of distance formula used\n",
    "\n",
    "- Euclidean\n",
    "- Hamming distance\n",
    "- Manhattan Distance\n",
    "\n",
    "- Euclidean\n",
    "\n",
    "The Euclidean distance between two points ‘p(p1,p2)’ and ‘q(q1,q2)’ is calculated as :\n",
    "\n",
    "p(x,y) = square root of (x1^2 - y1^2) + (x2^2-y^2)\n",
    "\n",
    "- Hamming distance\n",
    "\n",
    "\n",
    " hamming distance is a distance metric that measures the number of mismatches between two vectors. It is mostly used in the case of categorical data.\n",
    " Generally, if we have features as categorical data then we consider the difference to be 0 if both the values are the same and the difference is 1 if both the values are different.\n",
    " \n",
    "- Manhattan Distance\n",
    "\n",
    "\n",
    " This distance represents the sum of the absolute differences between the opposite values in vectors.\n",
    " \n",
    "     MD(x,y) = sum of i to n (|xi - yi|)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)\tWhat are Lazy Learners? Why KNN is called a lazy learner?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn’t create a generalized model for the training set but waits for the test data. Once test data is provided then only it starts generalizing the training data to classify the test data.It stores the training data and waits for the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)\tHow do we select the value of k? How bias and variance varies with k?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of k affects the k-NN classifier drastically.With lower value of ‘k’ variance is high and bias is low(overfitting)  but as we increase the value of ‘k’ variance starts decreasing and bias starts increasing(underfitting).\n",
    "So we should be to choose such value of ‘k’ for which we get a minimum of both the errors and avoid overfitting as well as underfitting.\n",
    "\n",
    "We use different ways to calculate the optimum value of ‘k’ such as cross validation, error versus k curve, checking accuracy for each value of ‘k’ etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)\tWhat are advantages and disadvantages of KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages :\n",
    "\n",
    "- It can be used for both regression and classification problems.\n",
    "- It is very simple and easy to implement.\n",
    "- Mathematics behind the algorithm is easy to understand.\n",
    "- There is no need to create model or do hyperparameter tuning.\n",
    "- KNN doesn't make any assumption for the distribution of the given data.\n",
    "- There is not much time cost in training phase.\n",
    "\n",
    "Distadvantages:\n",
    "\n",
    "- Finding the optimum value of ‘k’\n",
    "- It takes a lot of time to compute the distance between each test sample and all training samples.\n",
    "- Since the model is not saved beforehand in this algorithm (lazy learner), so every time one predicts a test value, it follows  the same steps again and again.\n",
    "- Since, we need to store the whole training set for every test set, it requires a lot of space.\n",
    "-It is not suitable for high dimensional data.\n",
    "- Expensive in testing phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7)\tDiscuss kDTree algorithm used for KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-d tree is a hierarchical binary tree. When this algorithm is used for k-NN classficaition, it rearranges the whole dataset in a binary tree structure, so that when test data is provided, it would give out the result by traversing through the tree, which takes less time than brute search.\n",
    "\n",
    "Once the tree is formed , it is easy for algorithm to search for the probable nearest neighbor just by traversing the tree. The main problem k-d trees is that it gives probable nearest neighbors but can miss out actual nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "8)\tDiscuss Ball Tree algorithm used for KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two clusters are created initially\n",
    "- All the data points must belong to atleast one of the clusters.\n",
    "- One point cannot be in both clusters.\n",
    "- Distance of the point is calculated from the centroid of the each cluster. The point closer to the centroid goes into that particular cluster.\n",
    "- Each cluster is then divided into sub clusters again, and then the points are classified into each cluster on the basis of distance from centroid.\n",
    "- This is how the clusters are kept to be divided till a certain depth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
