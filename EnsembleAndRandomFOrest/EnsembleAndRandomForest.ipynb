{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)\tWhat do you understand by Ensemble technique? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Endemble technique is a technique in which we have many predictors or group of predictors and we aggregate all the predictors result to acheive the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)\tExplain the idea behind ensemble techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea behind ensemble technique is that decrease the variance. As we know that a single predicor have low bias and high variance. In ensemble technique we use many predictors and when we aggregate the result of all the predictors then we get low variance.\n",
    "\n",
    "Suppose we we have ‘n’ predictors:\n",
    "\n",
    "Z1, Z2, Z3, ......., Zn with a standard deviation of σ\n",
    "\n",
    "Var(z) = σ^2\n",
    "\n",
    "If we use single predictors Z1, Z2, Z3, ......., Zn the variance associated with each will be σ2 but the expected value will be the average of all the predictors.\n",
    "\n",
    "Let’s consider the average of the predictors:\n",
    "\n",
    "µ = (Z1 + Z2 + Z3+.......+ Zn)/n\n",
    "\n",
    "if we use µ as the predictor then the expected value still remains the same but see the variance now:\n",
    "\n",
    "variance(µ) = σ^2/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\tWhat is Bootstrapping? How is sampling done in bootstrapping?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping is a technique of sampling different sets of data from a given training set by using replacement. Here we sampling done by randomly with replacement but some times this leads to out of bag(That means there might be some data which are never sampled at all)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)\tWhat is bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is the type of ensemble technique in which a single training algorithm is used on different samples of the training data where the sampling is done with replacement (bootstrap). Once the algorithm is trained on all the samples, then bagging makes the prediction by aggregating all the predictions made by the algorithm on different samples. In case of regression, bagging prediction is simply the mean of all the predictions and in the case of classifier, bagging prediction is the most frequent prediction (majority vote) among all the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)\tHow prediction is made in Bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In bagging technique we aggregate all the predictions made by the samples,In case of regression, bagging prediction is simply the mean of all the predictions and in the case of classifier, bagging prediction is the most frequent prediction (majority vote) among all the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)\tHow Ensemble technique solves the high variance issue with Decision trees?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that Decision tree is one of the algorithm that gives low bias and high variance. Sometimes it leads to overfitting condition. So bagging technique becomes a very good solution for decreasing the variance in a decision tree. As we know we use more predictors for tarining our dataset these predictors use decision tree, so as we use more than one decison trees as a result we get low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7)\tWhat is pasting? How is it different from bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pasting defines no replacement done while sampling the training dataset. It's similar to the bagging but in bagging we use random sampling with replacement.Pasting is also an ensemble technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8)\tWhat is Out Of Bag evaluation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In bagging as we perform random sampling with replacement sometimes this may happens some data are never sampled at all this is called Out Of Bag evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9)\tHow does a Random Forest model works?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is uses bagging techniques which few tweaks in algorithm and random forest uses Decsion tree algorithm in the predictors to train the model.\n",
    "\n",
    "1. Just like in bagging, different samples are collected from the training dataset using bootstraping.\n",
    "2.  On each sample we train our tree model and we allow the trees to grow with high depths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10)\tWhat is the difference between Bagging and Random forest? Why do we use Random forest more commonly than Bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diffrence between bagging and random forest is that, In Baggging we allow all the sample data to be used for splitting the nodes but in Random forest we are not using all the sample data here we perform both row and feature sampling.\n",
    "or in otherwords we can say that from total p predictors we only allow m predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11)\tWhat is feature sampling? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature sampling is theat from all the available features we select some features from them for training. It is the subset of al the feature availble in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12)\tHow prediction is made in Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest makes the prediction by taking the mode of all the predictions made by all the models, since this is the case of classification. This process is also known as “Majority voting”.In case of regression Random forest makes the prediction by taking the mean of all the predictions made by different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "13)\tWhen should we not use Random forest model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know Results of a random forest are very hard to interpret in comparison with decision trees and High computational time than other respective models.Random Forest should be used where accuracy is up utmost priority, if you are looking for interpretability then it can not be used because it's hard to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14)\tWhat is Stacking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking is a type of ensemble technique which combines the predictions of two or more models, also called base models, and use the combination as the input for a new model (meta-model) i.e. a new model is trained on the predictions of the base models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "15)\tExplain the working behind Stacking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for example lets we have a classification problem, The stacking techniques says that we use few models like KNN, SVM as the base model and make predictions using these models.Now the predictions made by these models are used as an input feature for Random forest to train on and give prediction.\n",
    "or in other words we can say stacking is an ensemble techniques, tries to improve the accuracy of a model by using predictions of not so good models and then using those predictions as an input feature for a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
