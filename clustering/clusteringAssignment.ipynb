{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is an unsupervised learning approach? Why is it needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning is a type of machine learning that looks for previously undetected patterns in a data set with no pre-existing labels and with a minimum of human supervision. \n",
    "Unsupervised methods help us to find features which can be useful for categorization.\n",
    "It is taken place in real time, so all the input data to be analyzed and labeled in the presence of learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWhat is clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method in which identify the similar instances and group them together is called Clustering.Clustering is an unsupervised approach which finds a structure/pattern in a collection of unlabeled data. A cluster is a collection of objects which are “similar” amongst themselves and are “dissimilar” to the objects belonging to a different cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tHow do clustering and classification differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustring is a Unsupervised learning approch, here we only have one set of input data (not labelled), about which we must obtain information, without previously knowing what the output will be. On the other hand Classification is a supervised learning approch in  which we have the input data having labels and we know the possible output of the algorithm. Here lebels responds to the problems by categorical answers like yes and no(Binary classification) or multiclassification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhat are the various applications of clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different type of clustering applications which are given below:\n",
    "- #### For customer segmentation:\n",
    "You can cluster your customers based on their purchases,their activity on your website, and so on. This is useful to understand who your customers are and what they need, so you can adapt your products and marketing campaigns to each segment. For example, this can be useful in recommender systems to suggest content that other users in the same cluster enjoyed.\n",
    "\n",
    "- #### For data analysis:\n",
    "When analyzing a new dataset, it is often useful to first discover clusters of similar instances, as it is often easier to analyze clusters separately.\n",
    "\n",
    "- #### For data analysis:\n",
    "Once a dataset has been clustered, it is usually possible to measure each instance’s affinity with each cluster (affinity is any measure of how well an instance fits into a cluster). Each instance’s feature vector x can then be replaced with the vector of its cluster affinities. If there are k clusters, then this vector is k dimensional. This is typically much lower dimensional than the original feature vector, but it can preserve enough information for further processing.\n",
    "- #### For semi-supervised learning:\n",
    "If you only have a few labels, you could perform clustering and propagate the labels to all the instances in the same cluster. This can greatly increase the amount of labels available for a subsequent supervised learning algorithm, and thus improve its performance.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tHow does clustering play a role in supervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is an unsupervised machine learning approach, but can it be used to improve the accuracy of supervised machine learning algorithms as well by clustering the data points into similar groups and using these cluster labels as independent variables in the supervised machine learning algorithm.\n",
    "- For a given given dataset we can perform the unsupervised learning on the dataset and make diffrent type of clusters on the given data and Try different type of supervised algorithm on those cluster. This approch improve our accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat are the requirements to be met by a clustering algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary requirements that should be met by a clustering algorithm are:\n",
    "* It should be scalable\n",
    "* It should be able to deal with attributes of different types;\n",
    "* It should be able to discover arbitrary shape clusters;\n",
    "* It should have an inbuilt ability to deal with noise and outliers;\n",
    "* The clusters should not vary with the order of input records;\n",
    "* It should be able to handle data of high dimensions.\n",
    "* It should be easy to interpret and use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tDiscuss the different approaches for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 types of clustering approches:\n",
    "\n",
    "- Agglomerative: \n",
    "This approach first considers all the points as individual clusters and then finds out the similarity between two points, puts them into a cluster. Then it goes on finding similar points and clusters until there is only one cluster left i.e., all points belong to a big cluster. This is also called the bottom-up approach.\n",
    "\n",
    "- Divisive: \n",
    "It is opposite of the agglomerative approach. It first considers all the points to be part of one big cluster and in the subsequent steps tries to find out the points/ clusters which are least similar to each other and then breaks the bigger cluster into smaller ones. This continues until there are as many clusters as there are datapoints. This is also called the top-down approach.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tWhat is WCSS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wcss stands for within-cluster sum of squared distances. It gives the mean squared distance between each instance and its closest centroid. It's also called as Inertia. Our target is to get minimum Wcss value to get a good cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tDiscuss the elbow method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is based on the relationship between the within-cluster sum of squared distances(WCSS Or Inertia) and the number of clusters. It is observed that first with an increase in the number of clusters WCSS decreases steeply and then after a certain number of clusters the drop in WCSS is not that prominent which helps to get optimum number of clusters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tWhat is the significance of ‘K’ in K-Means and how is it calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here K means no of cluster.It can be calculated by elbow method or we can give the random no of clusterrs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.\tDiscuss the step by step implementation of K-Means Clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Randomly assign K centres.\n",
    "2. Calculate the distance of all the points from all the K centres and allocate the points to cluster based on the shortest distance. The model’s _inertia_ is the mean squared distance between each instance and its closest centroid. The goal is to have a model with the lowes intertia.\n",
    "3. Once all the points are assigned to clusters, recompute the centroids.\n",
    "4. Repeat the steps 2 and 3 until the locations of the centroids stop changing and the cluster allocation of the points becomes constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.\tWhat are the challenges with K-Means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need to specify the number of clusters beforehand.\n",
    "- It is required to run the algorithm multiple times to avoid a sub-optimal solution\n",
    "- K-Means does not behave very well when the clusters have varying sizes, different densities, or non-spherical shapes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.\tDiscuss the various improvements in K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Yet another important variant of the K-Means algorithm was proposed in a 2010 paper by David Sculley.Instead of using the full dataset at each iteration, the algorithm is capable of using mini-batches, moving the centroids just slightly at each iteration. This speeds up the algorithm typically by a factor of 3 or 4 and makes it possible to cluster huge datasets that do not fit in memory. Scikit-Learn implements this algorithm in the MiniBatchKMeans class.\n",
    "- The clusters sometimes vary based on the initial choice of the centroids. An important improvement to the K-Means algorithm, called K-Means++, was proposed in a 2006 paper by David Arthur and Sergei Vassilvitskii. They introduced a smarter initialization step that tends to select centroids that are distant from one another, and this makes the K-Means algorithm much less likely to converge to a suboptimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering:\n",
    "14.\tDiscuss the agglomerative and divisive clustering approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglomerative: \n",
    "This approach first considers all the points as individual clusters and then finds out the similarity between two points, puts them into a cluster. Then it goes on finding similar points and clusters until there is only one cluster left i.e., all points belong to a big cluster. This is also called the bottom-up approach.\n",
    "\n",
    "Divisive: \n",
    "It is opposite of the agglomerative approach. It first considers all the points to be part of one big cluster and in the subsequent steps tries to find out the points/ clusters which are least similar to each other and then breaks the bigger cluster into smaller ones. This continues until there are as many clusters as there are datapoints. This is also called the top-down approach.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.\tWhat are dendrograms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dendrogram is a diagram that shows the hierarchical relationship between objects. It is most commonly created as an output from hierarchical clustering. The main use of a dendrogram is to work out the best way to allocate objects to clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16.\tDiscuss the Hierarchical clustering in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tBegin with n observations and a measure (such as Euclidean distance) of all the n(n−1)/2 pairwise dissimilarities(or the Euclidean distances generally). Treat each observation as its own cluster. Initially, we have n clusters.\n",
    "2.\tCompare all the distances and put the two closest points/clusters in the same cluster. The dissimilarity(or the Euclidean distances) between these two clusters indicates the height in the dendrogram at which the fusion line should be placed.\n",
    "3.\tCompute the new pairwise inter-cluster dissimilarities(or the Euclidean distances) among the remaining clusters.\n",
    "4.\tRepeat steps 2 and 3 till we have only one cluster left.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17.\tDiscuss the various linkage methods for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Single Linkage: cluster distance = smallest pairwise distance :\n",
    "    Minimal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities. Single linkage can result in extended, trailing clusters in which single observations are fused one-at-a-time.\n",
    "    \n",
    "    \n",
    "- Complete Linkage: cluster distance = largest pairwise distance\n",
    "    Maximal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities.\n",
    "    \n",
    "    \n",
    "- Average Linkage: cluster distance = average pairwise distance :\n",
    "    Mean intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.\n",
    "    \n",
    "    \n",
    "- Centroid Linkage: cluster distance=  distance between the centroids of the clusters :\n",
    "    The dissimilarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B. Centroid linkage can result in undesirable inversions.\n",
    "    \n",
    "    \n",
    "- Ward’s Linkage: cluster criteria= Minimize the variance in the cluster :\n",
    "    To implement this method, at each step find the pair of clusters that leads to minimum increase in total within-cluster variance after merging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18.\tDiscuss the differences between K-Means and Hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In KMeans we need to specify the number of clusters beforehand but in case of Hierarchical clustering we dont need to give the number of cluster.\n",
    "- K-Means does not behave very well when the clusters have varying sizes, different densities, or non-spherical shapes but which is not in case of Hierarchical clustering.\n",
    "- Hierarchical clustering can’t handle big data well but K Means clustering can but KMeans can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN\n",
    "19.\tDiscuss the basic terms used in DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon: \n",
    "This is also called eps. This is the distance till which we look for the neighbouring points.\n",
    "\n",
    "####  Min_points: \n",
    "The minimum number of points specified by the user.\n",
    "\n",
    "#### Core Points:\n",
    "If the number of points inside the eps radius of a point is greater than or equal to the min_points then it’s called a core point.\n",
    "\n",
    "####  Border Points:\n",
    "If the number of points inside the eps radius of a point is less than the min_points and it lies within the eps radius region of a core point, it’s called a border point.\n",
    "\n",
    "####  Noise:\n",
    "A point which is neither a core nor a border point is a noise point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20.\tDiscuss the step by step implementation of DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.\tThe algorithm starts with a random point in the dataset which has not been visited yet and its neighbouring points are identified based on the eps value.\n",
    "2.\tIf the point contains greater than or equal points than the min_pts, then the cluster formation starts and this point becomes a _core point_, else it’s considered as noise. The thing to note here is that a point initially classified as noise can later become a border point if it’s in the eps radius of a core point.\n",
    "3.\tIf the point is a core point, then all its neighbours become a part of the cluster. If the points in the neighbourhood turn out to be core points then their neighbours are also part of the cluster.\n",
    "4.\tRepeat the steps above until all points are  classified into different clusters or noises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cluster Evaluation\n",
    "21.\tWhat are the aspects of cluster validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- External: Compare your cluster to the ground truth.\n",
    "- Internal: Evaluating the cluster without reference to external data.\n",
    "- Reliability: The clusters are not formed by chance(randomly)- some statistical framework can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22.\tWhat is a confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23.\tWhat is Jaccard’s coefficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jaccard Coeeficient**=$\\frac{ SS}{(SS+SD+DS)}$\n",
    "\n",
    "where,\n",
    "P: ${P_1,P_2,…,P_m}$ the set of ground truth clusters\n",
    "C: ${C_1,C_2,…C_n}$ the set of clusters formed by the algorithm\n",
    "\n",
    "\n",
    "1.\t$C_{ij}=P_{ij}=1$ --> both the points belong to the same cluster for both our algorithm and ground truth(Agree)--- **SS**\n",
    "2.\t$C_{ij}=P_{ij}=0$ --> both the points don’t belong to the same cluster for both our algorithm and ground truth(Agree)--- **DD**\n",
    "3.\t$C_{ij}=1 but P_{ij}=0$ --> The points belong in the same cluster for our algorithm but in different clusters for the ground truth (Disagree)---- **SD**\n",
    "4.\t$C_{ij}=0 but P_{ij}=1$ --> The points don’t belong in the same cluster for our algorithm but in same clusters for the ground truth (Disagree)----**DS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24.\tWhat is Rand Index?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rand Index**= $\\frac{Total Agree}{Total Instances}=\\frac{(SS+DD)}{(SS+DD+DS+SD)}$\n",
    "\n",
    "sometimes it's value dominated by DD. If most of the data points are consider as the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25.\tWhat is the entropy of a cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy of Cluster i, given by $e_i= - \\sum p_{ij} log (p_{ij})$\n",
    "\n",
    "For the entire clustering algorithm, the entropy can be given as: \n",
    "$e= \\sum \\frac{m_i}{n}e_i$\n",
    "\n",
    "where,\n",
    "$n$= number of points\n",
    "\n",
    "$m_i$=points in _cluster i_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26.\tDiscuss the purity of a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purity is the total percentage of data points clustered correctly.\n",
    "\n",
    " The purity of  cluster i, given by $p_i=max (p_{ij})$ \n",
    "\n",
    "And for the entire cluster it is: $p(C)=\\sum \\frac{p_i}{n}$\n",
    "    \n",
    "where,\n",
    "$n$= number of points\n",
    "\n",
    "$m_i$=points in _cluster i_\n",
    "\n",
    "$c_j$=points in _class j_\n",
    "\n",
    "$n_{ij}$=  points in cluster i coming from cluster j\n",
    "\n",
    "$p_{ij}=\\frac{n_{ij}}{m_i}$= probability of element from cluster i to be assigned to class j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27.\tWhat are cohesion and compression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Cohesion:** How closely the objects in the same cluster are related to each other. It is the within-cluster sum of squared distances. It is the same metric that we used to calculate for the K-Means algorithm.\n",
    "$WCSS= \\sum \\sum (x-m_i)^2$\n",
    "\n",
    "* **Separation:** How different the objects in different clusters are and how distinct a well-separated cluster is from other clusters. It is the between cluster sum of squared distances. $BSS=\\sum C_i(m-m_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28.\tWhat are the steps for AWS deployment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Go to https://aws.amazon.com/ and create an account if already don’t have one.\n",
    "- Open a command prompt window, and install the AWS Elastic Beanstalk Command Line Interface by entering the command :\n",
    "        pip install awscli\n",
    "- On AWS Management Console go to IAM(Identity and Access Management), go to user and select add user.\n",
    "- Then add the ‘user name’, check ‘Programmatic access’ and then click ‘next: Permissions\n",
    "- Click ‘create group’ to create a new group.\n",
    "- Give the name of the ‘user group’, check the ‘AdministratorAccess’ policy and click ‘create group’\n",
    "- Click ‘next tags’.\n",
    "- Click ‘next: Reviews’ and then click ‘Create user’ to create the user\n",
    "- After the user gets created successfully, note down the ‘Access Key Id’ and the ‘Secret access key’ for future reference.\n",
    "- In the command prompt, navigate to the project folder and type the command eb init. \n",
    "- The application then asks for the region to be entered. After entering the region it gives an error saying that the user is not authorized. Enter the Access Key Id and secret access key copied here.\n",
    "- Select the project name from the list of projects shown or create a new project as desired and then select the python version to be used\n",
    "- Then provide the name for your repository and branch and disable ‘SSH’ for your application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29.\tWhat difficulties did you face while deploying to AWS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have hardcoded the hostname and port no so i face sme issue.\n",
    "then i use \n",
    "\n",
    "\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
