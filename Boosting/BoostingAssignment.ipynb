{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is an ensemble approach that starts from a weaker decision and keeps on building the models such that the final prediction is the weighted sum of all the weaker decision-makers. The weights are assigned based on the performance of an individual tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tHow do boosting and bagging differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In boosting technique we  build the models in stagewise way that means the model will learn from it's previous tree. which is not in bagging in bagging we perform resampling with replacement and train n trees simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tWhat are week and strong classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weak classifier is simply a classifier that performs poorly, but performs better than random guessing. Strong classifier perform better and gives accurate result combining the weaker classifier we can make strong classifier ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhy are trees deemed fit for boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- computational scalability,\n",
    "\n",
    "- handles missing values,\n",
    "\n",
    "- robust to outliers,\n",
    "\n",
    "- does not require feature scaling,\n",
    "\n",
    "- can deal with irrelevant inputs,\n",
    "\n",
    "- interpretable (if small),\n",
    "\n",
    "- handles mixed predictors as well (quantitive and qualitative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tExplain the step by step implementation of ADA Boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that the number of training samples is denoted by  𝑁 , and the number of iterations (created trees) is  𝑀 . Notice that possible class outputs are  𝑌={−1,1} \n",
    "Initialize the observation weights  𝑤𝑖=1𝑁  where  𝑖=1,2,…,𝑁  for all the samples.\n",
    "\n",
    "For  𝑚=1  to  𝑀 :\n",
    "\n",
    "fit a classifier  𝐺𝑚(𝑥)  to the training data using weights  𝑤𝑖 ,\n",
    "\n",
    "compute  𝑒𝑟𝑟𝑚=∑𝑁𝑖=1𝑤𝑖𝐼(𝑦𝑖≠𝐺𝑚(𝑥))∑𝑁𝑖=1𝑤𝑖 ,\n",
    "\n",
    "compute  𝛼𝑚=12log((1−𝑒𝑟𝑟𝑚)𝑒𝑟𝑟𝑚) . This is the contribution of that tree to the final result.\n",
    "\n",
    "calculate the new weights using the formula:\n",
    "\n",
    "𝑤𝑖←𝑤𝑖⋅exp[𝛼𝑚⋅𝐼(𝑦𝑖≠𝐺𝑚(𝑥)] , where  𝑖=1,2,…,𝑁 \n",
    "Normalize the new sample weights so that their sum is 1.\n",
    "\n",
    "Construct the next tree using the new weights\n",
    "\n",
    "At the end, compare the summation of results from all the trees and the final result is either the one with the highest sum(for regression) or it is the class which has the most weighted voted average(for classification).\n",
    "\n",
    "Output  𝐺𝑚(𝑥)=𝑎𝑟𝑔𝑚𝑎𝑥[∑𝑀𝑚=1𝛼𝑚𝐺𝑚(𝑥)]  (Regression)\n",
    "\n",
    "Output  𝐺𝑚(𝑥)=𝑠𝑖𝑔𝑚[∑𝑀𝑚=1𝛼𝑚𝐺𝑚(𝑥)]  (Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat are pseudo residuals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pseudo residuals is the diffrence between the actual target column and the predicted target column(i.e which is average of the target column in the first iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tExplain the step by step implementation of Gradient boosted trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average of the label column as initially this average shall minimise the total error.\n",
    "\n",
    "Calculate the pseudo residuals.\n",
    "\n",
    " Pseudo residual= actual label- the predicted result (which is average in the first iteration)\n",
    "Mathematically,\n",
    "\n",
    "derivative of the pseudo residual= 𝛿𝐿(𝑦𝑖,𝑓(𝑥𝑖)) /𝛿(𝑓(𝑥𝑖)) \n",
    "where, L is the loss function.\n",
    "\n",
    " Here, the gradient of the error term is getting calculated as the goal is to minimize the error. Hence the name gradient boosted trees\n",
    "create a tree to predict the pseudo residuals instead of a tree to predict for the actual column values.\n",
    "\n",
    "new result= previous result+learning rate* residual\n",
    "\n",
    "Mathematically,  𝐹1(𝑥)=𝐹0(𝑥)+𝜈∑𝛾 \n",
    "where  𝜈  is the learning rate and  𝛾  is the residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tExplain the step by step implementation of XGBoost Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost's objective function is the sum of loss function evaluated over all the predictions and a regularisation function for all predictors ($j$ trees). In the formula $f_j$ means a prediction coming from the $j^th$ tree.\n",
    "\n",
    "$$\n",
    "obj(\\theta) = \\sum_{i}^{n} l(y_i - \\hat{y_i}) +  \\sum_{j=1}^{j} \\Omega (f_j)\n",
    "$$\n",
    "\n",
    "Loss function depends on the task being performed (classification, regression, etc.) and a regularization term is described by the following equation:\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2\n",
    "$$\n",
    "\n",
    "First part ($\\gamma T$) is responsible for controlling the overall number of created leaves, and the second term ($\\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2$) watches over the scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mathematics Involved**\n",
    "Unlike the other tree-building algorithms, XGBoost doesn’t use entropy or Gini indices. Instead, it utilises gradient (the error term) and hessian for creating the trees. Hessian for a Regression problem is the *number of residuals* and for a classification problem. Mathematically, Hessian is a second order derivative of the loss at the current estimate given as:\n",
    "\n",
    "<img src=\"1.PNG\" width=\"300\">\n",
    "\n",
    "where **L** is the loss function. \n",
    "\n",
    "- Initialise the tree with only one leaf.\n",
    "- compute the similarity using the formula\n",
    "$$\n",
    "Similarity= \\frac {Gradient^2}{ hessian +\\lambda}\n",
    "$$\n",
    "Where $\\lambda $ is the regularisation term.\n",
    "- Now for splitting data into a tree form, calculate\n",
    "$$\n",
    "Gain=  left similarity+right similarity-similarity for root\n",
    "$$ \n",
    "- For tree pruning, the parameter $ \\gamma$ is used. The algorithm starts from the lowest level of the tree and then starts pruning based on the value of $\\gamma$.\n",
    "\n",
    "\n",
    " If $Gain- \\gamma < 0$, remove that branch. Else, keep the branch \n",
    " \n",
    "- Learning is done using the equation\n",
    "$$\n",
    "New Value= old Value+ \\eta * prediction\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat are the advantages of XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out of the box feature of appropriate bias-variance trade-off,\n",
    "\n",
    "great computation speed as it utilises parallel computing and cache optimization,\n",
    "\n",
    "uses hardware optimization,\n",
    "\n",
    "works well even if the features are correlated\n",
    "\n",
    "robust even if there is noise for classification problem\n",
    "\n",
    "the facility of early stopping\n",
    "\n",
    "the package is evolving, i.e., new features are being added.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
