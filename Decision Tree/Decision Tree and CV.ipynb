{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)\tHow decision tree works for a regression problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing regression with a decision tree, we try to divide the given values of X into distinct and non-overlapping regions.\n",
    "for example :-  for a set of possible values X1, X2,..., Xp; we will try to divide them into J distinct and non-overlapping regions R1, R2, . . . , RJ. RJ. For a given observation falling into the region Rj, the prediction is equal to the mean of the response(y) values for each training observations(x) in the region Rj."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)\tWhy Recursive binary splitting is called Greedy Approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we try to divide the X values into j regions, but it is very expensive in terms of computational time to try to fit every set of X values into j regions. Thus, decision tree opts for a top-down greedy approach in which nodes are divided into two regions based on the given condition, i.e. not every node will be split but the ones which satisfy the condition are split into two branches. It is called greedy because it does the best split at a given step at that point of time rather than looking for splitting a step for a better tree in upcoming steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\tWhat do you understand by Greedy approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy is an algorithmic paradigm that builds up a solution piece by piece, always choosing the next piece that offers the most obvious and immediate benefit at that time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)\tWhat is Pruning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree pruning is the method of trimming down a full tree to reduce the complexity and variance in the data. we just regularise the decision tree by adding some new term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)\tWhat’s the difference between pre pruning and post pruning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-pruning, also known as backward pruning, is the process where the decision tree is generated first and then the non-significant branches are removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)\tWhat is Entropy? How is it calculated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is the measure of randomness in the data. In other words, it gives the impurity present in the dataset.\n",
    "\n",
    " Let’s suppose there are ‘m’ observations and we need to classify them into categories 1 and 2. Let’s say that category 1 has ‘n’ observations and category 2 has ‘m-n’ observations.\n",
    "\n",
    "p= n/m and q = m-n/m = 1-p\n",
    "\n",
    "then, entropy for the given set is:\n",
    "\n",
    "      E = -p*log2(p) – q*log2(q) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7)\tWhat is Gini Impurity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labelled if it was randomly labelled according to the distribution of labels in the subset.\n",
    "Ginni impurity value lies between 0 and 1, 0 being no impurity and 1 denoting random distribution. The node for which the Ginni impurity is least is selected as the root node to split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8)\tWhat do you understand by Information Gain? How does it help in tree building?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information gain calculates the decrease in entropy after splitting a node. It is the difference between entropies before and after the split. The more the information gain, the more entropy is removed.\n",
    "\n",
    "G(T,X) = Entropy(T) - Entropy(T,X)\n",
    "\n",
    "Where, T is the parent node before split and X is the split node from T."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9)\tHow does node selection take place while building a tree?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate gini impurity for each of the features and then select that feature which has least gini impurity or entropy. then we calculate the information gain of each feature and which feature giving highest information gain then that feture selected as root node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10)\tWhat are different algorithms available for decision tree?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID3 (Iterative Dichotomiser) and Classfication and Regression Algorithm(CART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11)\tWhat’s the main difference between Gini Impurity and Entropy on the basis of computation time?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini Impurity and Entropy both are doing the same work the difference is that gini is easy to calculate  since entropy has a log term calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12)\tWhat are the disadvantages and advantages of using a Decision Tree?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disadvantages :\n",
    "\n",
    "- A small change in data can cause instability in the model because of the greedy approach.\n",
    "- Probability of overfitting is very high for Decision Trees.\n",
    "- It takes more time to train a decision tree model than other classification algorithms.\n",
    "\n",
    "#### Advantages :\n",
    "\n",
    "- It can be used for both Regression and Classification problems.\n",
    "- Decision Trees are very easy to grasp as the rules of splitting is clearly mentioned.\n",
    "- Complex decision tree models are very simple when visualized. It can be understood just by visualising.\n",
    "- Scaling and normalization are not needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13)\tHow do you deploy model in Heroku?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add a file called ‘gitignore’ inside the project folder. This folder contains the list of the files which we don’t want to include in the git repository. \n",
    "\n",
    "- Add a file called ‘Procfile’ inside the ‘reviewScrapper’ folder. This folder contains the command to run the flask application once deployed on the server:\n",
    "\n",
    " web: gunicorn app:app\n",
    "- create requirements.txt file by this command pip freeze > requirements.txt\n",
    "\n",
    "- After installing the Heroku CLI, Open a command prompt window and navigate to your project folder.\n",
    "\n",
    "- Type the command heroku login to login to your heroku account.\n",
    "- After logging in to Heroku, enter the command heroku create to create a heroku app. It will give you the URL of your Heroku app after successful creation. Or alternatively, you can go to the heroku website and create an app directly.\n",
    "\n",
    "- Before deploying the code to the Heroku cloud, we need to commit the changes to the git repository.\n",
    "- Type the command git init to initialize a local git repository.\n",
    "- Enter the command git status to see the uncommitted changes.\n",
    "- Enter the command git add . to add the uncommitted changes to the local repository.\n",
    "- Enter the command git commit -am \"make it better\" to commit the changes to the local repository.\n",
    "- Enter the command git push heroku master to push the code to the heroku cloud.\n",
    "- After deployment, heroku gives you the URL to hit the web API.\n",
    "- Once your application is deployed successfully, enter the command heroku logs --tail to see the logs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14)\tWhat challenges you faced while deploying the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procfile is not created properly.\n",
    "\n",
    "wrong name for requirements.txt file i face the problems.\n",
    "\n",
    "In Django application settings.py allowed host section i had made some changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)\tWhat is Cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a resampling technique with a basic idea of dividing the training dataset into two parts i.e. train and test. On one part(train) you try to train the model and on the second part(test) i.e. the data which is unseen for the model, you make the prediction and check how well your model works on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)\tWhy do we need to implement Cross validation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes our model predicted good in training data and gave good accuracy but in case of unseen data or test data the model will not perform well and give the bad accuracy this is nothing but overfitting, To avoid these kind of situation we use cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\tWhat are different types of CV methods?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holdout, k-fold, Leave One Out Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4)\tHow bias and variance varies for each CV method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that kfold gives the some biased results than that of Leave One Out Cross Validation and on the other hand kfold gives less variance than that of Leave One Out Cross Validation. so while considering a cross validation method we choose a method that  gives both low bias and low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5)\tIs Train Test Split a kind of CV? True or False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both are spliting the dataset for training and testing but CV uses resampling technique to split the data by using some CV methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "6)\tHow can we check over fitting using CV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the choosen CV method gives high variance in test datset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
